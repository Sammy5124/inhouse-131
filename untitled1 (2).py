# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15I07o0LkYG0tCU24-l9WQCnaGfam41Ul
"""

# Commented out IPython magic to ensure Python compatibility.
# Import libraries. You may or may not use all of these.
!pip install -q git+https://github.com/tensorflow/docs
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling

# Import data
!wget https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv
dataset = pd.read_csv('insurance.csv')
dataset.tail()

len(dataset)

dataset["sex"] = pd.factorize(dataset["sex"])[0]
dataset['smoker']=pd.factorize(dataset['smoker'])[0]
dataset['region']=pd.factorize(dataset['region'])[0]

test_data=dataset.sample(frac=0.2)
len(test_data)

train_data = dataset[~dataset.isin(test_data)].dropna()
len(train_data)

train_label=train_data.pop('expenses')
test_label=test_data.pop('expenses')
train_label
test_label

normalizer = layers.experimental.preprocessing.Normalization()
normalizer.adapt(np.array(train_data))

from keras.models import Sequential, Model
from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, Conv2D, MaxPooling2D

# Define the model
model = keras.Sequential()
model.add(normalizer)
model.add(Dense(16, activation='relu'))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(1))
model.summary()

model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.1),
    loss='mae',
    metrics=['mae', 'mse']
)

history = model.fit(
    train_data,
    train_label,
    epochs=100,
    validation_split=0.5,
    verbose=0, # disable logging
)

print(history)

# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.
# Test model by checking how well the model generalizes using the test set.
loss, mae, mse = model.evaluate(test_data, test_label, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} expenses".format(mae))

if mae < 3500:
  print("You passed the challenge. Great job!")
else:
  print("The Mean Abs Error must be less than 3500. Keep trying.")

# Plot predictions.
test_predictions = model.predict(test_data).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_label, test_predictions)
plt.xlabel('True values (expenses)')
plt.ylabel('Predictions (expenses)')
lims = [0, 50000]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims,lims)

"""Project number 131"""

# Commented out IPython magic to ensure Python compatibility.
# Import libraries. You may or may not use all of these.
!pip install -q git+https://github.com/tensorflow/docs
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling

# Import data
!wget https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv
dataset = pd.read_csv('insurance.csv')
dataset.tail()

len(dataset)

dataset["sex"] = pd.factorize(dataset["sex"])[0]
dataset['smoker']=pd.factorize(dataset['smoker'])[0]
dataset['region']=pd.factorize(dataset['region'])[0]

test_data=dataset.sample(frac=0.2)
len(test_data)

train_data = dataset[~dataset.isin(test_data)].dropna()
len(train_data)

train_label=train_data.pop('expenses')
test_label=test_data.pop('expenses')
train_label
test_label

normalizer = layers.experimental.preprocessing.Normalization()
normalizer.adapt(np.array(train_data))

from keras.models import Sequential, Model
from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D, Conv2D, MaxPooling2D

# Define the model
model = keras.Sequential()
model.add(normalizer)
model.add(Dense(64, activation='relu'))  # Increase neurons in the first layer
model.add(Dense(32, activation='relu'))  # Add another hidden layer
model.add(layers.Dropout(0.3))  # Adjust dropout rate
model.add(layers.Dense(1))
model.summary()

model.compile(
    optimizer=tf.optimizers.Adam(learning_rate=0.1),
    loss='mae',
    metrics=['mae', 'mse']
)

history = model.fit(
    train_data,
    train_label,
    epochs=100,
    validation_split=0.5,
    verbose=0, # disable logging
)

print(history)

# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.
# Test model by checking how well the model generalizes using the test set.
loss, mae, mse = model.evaluate(test_data, test_label, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} expenses".format(mae))

if mae < 3500:
  print("You passed the challenge. Great job!")
else:
  print("The Mean Abs Error must be less than 3500. Keep trying.")

# Plot predictions.
test_predictions = model.predict(test_data).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_label, test_predictions)
plt.xlabel('True values (expenses)')
plt.ylabel('Predictions (expenses)')
lims = [0, 50000]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims,lims)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Reshape the data for scikit-learn
X_train_reshaped = np.array(train_data).reshape(-1, train_data.shape[1])
X_test_reshaped = np.array(test_data).reshape(-1, test_data.shape[1])

# Create a Linear Regression model
linear_model = LinearRegression()

# Fit the model to the training data
linear_model.fit(X_train_reshaped, train_label)

# Make predictions on the test data
linear_predictions = linear_model.predict(X_test_reshaped)

# Evaluate the model
linear_mse = mean_squared_error(test_label, linear_predictions)
linear_r2 = r2_score(test_label, linear_predictions)

print("Linear Regression Mean Squared Error:", linear_mse)
print("Linear Regression R-squared:", linear_r2)

# Plot the results
plt.scatter(test_label, linear_predictions)
plt.xlabel("True Values")
plt.ylabel("Predictions")
plt.title("Linear Regression Results")

# Add a diagonal line for reference
min_val = min(min(test_label), min(linear_predictions))
max_val = max(max(test_label), max(linear_predictions))
plt.plot([min_val, max_val], [min_val, max_val], 'k--')

plt.show()

# Function to predict cost
def predict_cost(input_data):
  # Ensure input_data is in the correct format (NumPy array)
  input_array = np.array([input_data])

  # Make prediction
  prediction = model.predict(input_array)

  # Return the predicted cost
  return prediction[0][0]

# Example usage
new_data = [25, 1, 27.9, 0, 1, 1]  # Example input data
predicted_cost = predict_cost(new_data)
print("Predicted cost:", predicted_cost)

# Function to predict cost with user input
def predict_cost():
  age = float(input("Enter age: "))
  sex = int(input("Enter sex (0 or 1): "))
  bmi = float(input("Enter BMI: "))
  children = int(input("Enter number of children: "))
  smoker = int(input("Enter smoker status (0 or 1): "))
  region = int(input("Enter region (0, 1, 2, or 3): "))

  input_data = [age, sex, bmi, children, smoker, region]
  input_array = np.array([input_data])

  prediction = model.predict(input_array)
  print("Predicted cost: Rs.", prediction[0][0])

# Call the function to start the prediction process
predict_cost()

"""Code 1 (Without User Input):
Objective: Provide a function to predict cost given a pre-defined set of input features.
Gap: The input data is hardcoded, limiting flexibility for different predictions.


Code 2 (With User Input):
Objective: Create an interactive program that takes user input for features and predicts the cost accordingly.
Gap: The previous code's limitation of fixed input is addressed by enabling user interaction.

Key Differences:

Input Handling: Code 1 relies on a fixed input list, while Code 2 dynamically collects input from the user.
Flexibility: Code 2 is more flexible, allowing users to predict costs for different scenarios without modifying the code.
User Experience: Code 2 provides a more interactive and user-friendly experience.


In summary: Code 1 is suitable for a one-off prediction with known input, while Code 2 is designed for repeated predictions with varying user-provided inputs.
"""